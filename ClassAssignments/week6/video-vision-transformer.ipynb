{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/vivit.ipynb","timestamp":1739776561805}],"gpuType":"V28"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Video Vision Transformer\n\n**Author:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Ayush Thakur](https://twitter.com/ayushthakur0) (equal contribution)<br>\n**Date created:** 2022/01/12<br>\n**Last modified:**  2024/01/15<br>\n**Description:** A Transformer-based architecture for video classification.","metadata":{"id":"tP0ObzhPw4HY"}},{"cell_type":"markdown","source":"## Introduction\n\nVideos are sequences of images. Let's assume you have an image\nrepresentation model (CNN, ViT, etc.) and a sequence model\n(RNN, LSTM, etc.) at hand. We ask you to tweak the model for video\nclassification. The simplest approach would be to apply the image\nmodel to individual frames, use the sequence model to learn\nsequences of image features, then apply a classification head on\nthe learned sequence representation.\nThe Keras example\n[Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/)\nexplains this approach in detail. Alernatively, you can also\nbuild a hybrid Transformer-based model for video classification as shown in the Keras example\n[Video Classification with Transformers](https://keras.io/examples/vision/video_transformers/).\n\nIn this example, we minimally implement\n[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)\nby Arnab et al., a **pure Transformer-based** model\nfor video classification. The authors propose a novel embedding scheme\nand a number of Transformer variants to model video clips. We implement\nthe embedding scheme and one of the variants of the Transformer\narchitecture, for simplicity.\n\nThis example requires  `medmnist` package, which can be installed\nby running the code cell below.","metadata":{"id":"-yn9eTSxw4Hg"}},{"cell_type":"code","source":"!pip install -qq medmnist","metadata":{"id":"ReUXtb36w4Hi","executionInfo":{"status":"ok","timestamp":1739933249253,"user_tz":-420,"elapsed":1430,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:04:56.954031Z","iopub.execute_input":"2025-02-19T03:04:56.954292Z","iopub.status.idle":"2025-02-19T03:05:06.053150Z","shell.execute_reply.started":"2025-02-19T03:04:56.954267Z","shell.execute_reply":"2025-02-19T03:05:06.051620Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"jNm0zd28w4Hl"}},{"cell_type":"code","source":"import os\nimport io\nimport imageio\nimport medmnist\nimport ipywidgets\nimport numpy as np\nimport tensorflow as tf  # for data preprocessing only\nimport keras\nfrom keras import layers, ops\n\n# Setting seed for reproducibility\nSEED = 42\nos.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\nkeras.utils.set_random_seed(SEED)","metadata":{"id":"dnskuJdrw4Hl","executionInfo":{"status":"ok","timestamp":1739932235955,"user_tz":-420,"elapsed":67250,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:06.054335Z","iopub.execute_input":"2025-02-19T03:05:06.054641Z","iopub.status.idle":"2025-02-19T03:05:26.856336Z","shell.execute_reply.started":"2025-02-19T03:05:06.054614Z","shell.execute_reply":"2025-02-19T03:05:26.854957Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Hyperparameters\n\nThe hyperparameters are chosen via hyperparameter\nsearch. You can learn more about the process in the \"conclusion\" section.","metadata":{"id":"RrWRqGBMw4Hm"}},{"cell_type":"code","source":"# DATA\nDATASET_NAME = \"organmnist3d\"\nBATCH_SIZE = 32\nAUTO = tf.data.AUTOTUNE\nINPUT_SHAPE = (28, 28, 28, 1)\nNUM_CLASSES = 11\n\n# OPTIMIZER\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\n\n# TRAINING\nEPOCHS = 60\n\n# TUBELET EMBEDDING\nPATCH_SIZE = (8, 8, 8)\nNUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n\n# ViViT ARCHITECTURE\nLAYER_NORM_EPS = 1e-6\nPROJECTION_DIM = 128\nNUM_HEADS = 8\nNUM_LAYERS = 8","metadata":{"id":"j6dEscWbw4Hn","executionInfo":{"status":"ok","timestamp":1739932235963,"user_tz":-420,"elapsed":3,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:26.857589Z","iopub.execute_input":"2025-02-19T03:05:26.858222Z","iopub.status.idle":"2025-02-19T03:05:26.864356Z","shell.execute_reply.started":"2025-02-19T03:05:26.858192Z","shell.execute_reply":"2025-02-19T03:05:26.863236Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Dataset\n\nFor our example we use the\n[MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification](https://medmnist.com/)\ndataset. The videos are lightweight and easy to train on.","metadata":{"id":"_ccWaqa-w4Ho"}},{"cell_type":"code","source":"\ndef download_and_prepare_dataset(data_info: dict):\n    \"\"\"Utility function to download the dataset.\n\n    Arguments:\n        data_info (dict): Dataset metadata.\n    \"\"\"\n    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n\n    with np.load(data_path) as data:\n        # Get videos\n        train_videos = data[\"train_images\"]\n        valid_videos = data[\"val_images\"]\n        test_videos = data[\"test_images\"]\n\n        # Get labels\n        train_labels = data[\"train_labels\"].flatten()\n        valid_labels = data[\"val_labels\"].flatten()\n        test_labels = data[\"test_labels\"].flatten()\n\n    return (\n        (train_videos, train_labels),\n        (valid_videos, valid_labels),\n        (test_videos, test_labels),\n    )\n\n\n# Get the metadata of the dataset\ninfo = medmnist.INFO[DATASET_NAME]\n\n# Get the dataset\nprepared_dataset = download_and_prepare_dataset(info)\n(train_videos, train_labels) = prepared_dataset[0]\n(valid_videos, valid_labels) = prepared_dataset[1]\n(test_videos, test_labels) = prepared_dataset[2]","metadata":{"id":"RHW2yL5-w4Hp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739932238926,"user_tz":-420,"elapsed":2961,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"outputId":"7592c6ea-5571-4d8c-f737-efdacfbfeb8b","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:26.866978Z","iopub.execute_input":"2025-02-19T03:05:26.867426Z","iopub.status.idle":"2025-02-19T03:05:31.886468Z","shell.execute_reply.started":"2025-02-19T03:05:26.867382Z","shell.execute_reply":"2025-02-19T03:05:31.885199Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://zenodo.org/records/10519652/files/organmnist3d.npz?download=1\n\u001b[1m32657349/32657349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### `tf.data` pipeline","metadata":{"id":"gczoiprFw4Hq"}},{"cell_type":"code","source":"\ndef preprocess(frames: tf.Tensor, label: tf.Tensor):\n    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n    # Preprocess images\n    frames = tf.image.convert_image_dtype(\n        frames[\n            ..., tf.newaxis\n        ],  # The new axis is to help for further processing with Conv3D layers\n        tf.float32,\n    )\n    # Parse label\n    label = tf.cast(label, tf.float32)\n    return frames, label\n\n\ndef prepare_dataloader(\n    videos: np.ndarray,\n    labels: np.ndarray,\n    loader_type: str = \"train\",\n    batch_size: int = BATCH_SIZE,\n):\n    \"\"\"Utility function to prepare the dataloader.\"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n\n    if loader_type == \"train\":\n        dataset = dataset.shuffle(BATCH_SIZE * 2)\n\n    dataloader = (\n        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n        .batch(batch_size)\n        .prefetch(tf.data.AUTOTUNE)\n    )\n    return dataloader\n\n\ntrainloader = prepare_dataloader(train_videos, train_labels, \"train\")\nvalidloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\ntestloader = prepare_dataloader(test_videos, test_labels, \"test\")","metadata":{"id":"nIORkEQTw4Hr","executionInfo":{"status":"ok","timestamp":1739932245516,"user_tz":-420,"elapsed":6586,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:31.887956Z","iopub.execute_input":"2025-02-19T03:05:31.888275Z","iopub.status.idle":"2025-02-19T03:05:32.133233Z","shell.execute_reply.started":"2025-02-19T03:05:31.888247Z","shell.execute_reply":"2025-02-19T03:05:32.132222Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Tubelet Embedding\n\nIn ViTs, an image is divided into patches, which are then spatially\nflattened, a process known as tokenization. For a video, one can\nrepeat this process for individual frames. **Uniform frame sampling**\nas suggested by the authors is a tokenization scheme in which we\nsample frames from the video clip and perform simple ViT tokenization.\n\n| ![uniform frame sampling](https://i.imgur.com/aaPyLPX.png) |\n| :--: |\n| Uniform Frame Sampling [Source](https://arxiv.org/abs/2103.15691) |\n\n**Tubelet Embedding** is different in terms of capturing temporal\ninformation from the video.\nFirst, we extract volumes from the video -- these volumes contain\npatches of the frame and the temporal information as well. The volumes\nare then flattened to build video tokens.\n\n| ![tubelet embedding](https://i.imgur.com/9G7QTfV.png) |\n| :--: |\n| Tubelet Embedding [Source](https://arxiv.org/abs/2103.15691) |","metadata":{"id":"CWtlCf3ww4Hs"}},{"cell_type":"code","source":"\nclass TubeletEmbedding(layers.Layer):\n    def __init__(self, embed_dim, patch_size, **kwargs):\n        super().__init__(**kwargs)\n        self.projection = layers.Conv3D(\n            filters=embed_dim,\n            kernel_size=patch_size,\n            strides=patch_size,\n            padding=\"VALID\",\n        )\n        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n\n    def call(self, videos):\n        projected_patches = self.projection(videos)\n        flattened_patches = self.flatten(projected_patches)\n        return flattened_patches\n","metadata":{"id":"Pjtxj7Edw4Hs","executionInfo":{"status":"ok","timestamp":1739932245522,"user_tz":-420,"elapsed":3,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:32.134175Z","iopub.execute_input":"2025-02-19T03:05:32.134472Z","iopub.status.idle":"2025-02-19T03:05:32.140369Z","shell.execute_reply.started":"2025-02-19T03:05:32.134448Z","shell.execute_reply":"2025-02-19T03:05:32.139342Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Positional Embedding\n\nThis layer adds positional information to the encoded video tokens.","metadata":{"id":"TS8jpLRKw4Ht"}},{"cell_type":"code","source":"\nclass PositionalEncoder(layers.Layer):\n    def __init__(self, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n    def build(self, input_shape):\n        _, num_tokens, _ = input_shape\n        self.position_embedding = layers.Embedding(\n            input_dim=num_tokens, output_dim=self.embed_dim\n        )\n        self.positions = ops.arange(0, num_tokens, 1)\n\n    def call(self, encoded_tokens):\n        # Encode the positions and add it to the encoded tokens\n        encoded_positions = self.position_embedding(self.positions)\n        encoded_tokens = encoded_tokens + encoded_positions\n        return encoded_tokens\n","metadata":{"id":"8bRoqH55w4Ht","executionInfo":{"status":"ok","timestamp":1739932245526,"user_tz":-420,"elapsed":2,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:32.141358Z","iopub.execute_input":"2025-02-19T03:05:32.141777Z","iopub.status.idle":"2025-02-19T03:05:32.163534Z","shell.execute_reply.started":"2025-02-19T03:05:32.141740Z","shell.execute_reply":"2025-02-19T03:05:32.162493Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Video Vision Transformer\n\nThe authors suggest 4 variants of Vision Transformer:\n\n- Spatio-temporal attention\n- Factorized encoder\n- Factorized self-attention\n- Factorized dot-product attention\n\nIn this example, we will implement the **Spatio-temporal attention**\nmodel for simplicity. The following code snippet is heavily inspired from\n[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\nOne can also refer to the\n[official repository of ViViT](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)\nwhich contains all the variants, implemented in JAX.","metadata":{"id":"45kIKTCsw4Hu"}},{"cell_type":"code","source":"\ndef create_vivit_classifier(\n    tubelet_embedder,\n    positional_encoder,\n    input_shape=INPUT_SHAPE,\n    transformer_layers=NUM_LAYERS,\n    num_heads=NUM_HEADS,\n    embed_dim=PROJECTION_DIM,\n    layer_norm_eps=LAYER_NORM_EPS,\n    num_classes=NUM_CLASSES,\n):\n    # Get the input layer\n    inputs = layers.Input(shape=input_shape)\n    # Create patches.\n    patches = tubelet_embedder(inputs)\n    # Encode patches.\n    encoded_patches = positional_encoder(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization and MHSA\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n        )(x1, x1)\n\n        # Skip connection\n        x2 = layers.Add()([attention_output, encoded_patches])\n\n        # Layer Normalization and MLP\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        x3 = keras.Sequential(\n            [\n                layers.Dense(units=embed_dim * 4, activation=ops.gelu),\n                layers.Dense(units=embed_dim, activation=ops.gelu),\n            ]\n        )(x3)\n\n        # Skip connection\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Layer normalization and Global average pooling.\n    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n    representation = layers.GlobalAvgPool1D()(representation)\n\n    # Classify outputs.\n    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model\n","metadata":{"id":"EmdNsiSqw4Hv","executionInfo":{"status":"ok","timestamp":1739932245543,"user_tz":-420,"elapsed":14,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:32.164699Z","iopub.execute_input":"2025-02-19T03:05:32.165020Z","iopub.status.idle":"2025-02-19T03:05:32.189227Z","shell.execute_reply.started":"2025-02-19T03:05:32.164989Z","shell.execute_reply":"2025-02-19T03:05:32.188104Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Train","metadata":{"id":"fv9jpSPFw4Hv"}},{"cell_type":"code","source":"def run_experiment():\n    # Initialize model\n    model = create_vivit_classifier(\n        tubelet_embedder=TubeletEmbedding(\n            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n        ),\n        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n    )\n\n    # Compile the model with the optimizer, loss function\n    # and the metrics.\n    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(\n        optimizer=optimizer,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\n            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n        ],\n    )\n\n    # Train the model.\n    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n\n    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n    return model\n\n\nmodel = run_experiment()","metadata":{"id":"tkJTd8Muw4Hw","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1739933956677,"user_tz":-420,"elapsed":22414,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"outputId":"0a9119d7-1272-4dd8-ae12-767b47227e79","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:05:32.190161Z","iopub.execute_input":"2025-02-19T03:05:32.190565Z","iopub.status.idle":"2025-02-19T03:18:07.421818Z","shell.execute_reply.started":"2025-02-19T03:05:32.190529Z","shell.execute_reply":"2025-02-19T03:18:07.420823Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 456ms/step - accuracy: 0.1159 - loss: 2.5143 - top-5-accuracy: 0.5711 - val_accuracy: 0.1304 - val_loss: 2.2312 - val_top-5-accuracy: 0.6149\nEpoch 2/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 384ms/step - accuracy: 0.1752 - loss: 2.1969 - top-5-accuracy: 0.6877 - val_accuracy: 0.4224 - val_loss: 1.8634 - val_top-5-accuracy: 0.7826\nEpoch 3/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 387ms/step - accuracy: 0.2812 - loss: 2.0115 - top-5-accuracy: 0.7904 - val_accuracy: 0.2919 - val_loss: 1.7207 - val_top-5-accuracy: 0.8882\nEpoch 4/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 376ms/step - accuracy: 0.2834 - loss: 1.8543 - top-5-accuracy: 0.8255 - val_accuracy: 0.4161 - val_loss: 1.3810 - val_top-5-accuracy: 0.9441\nEpoch 5/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 392ms/step - accuracy: 0.3880 - loss: 1.6148 - top-5-accuracy: 0.8896 - val_accuracy: 0.4720 - val_loss: 1.3468 - val_top-5-accuracy: 0.9255\nEpoch 6/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 402ms/step - accuracy: 0.4362 - loss: 1.4635 - top-5-accuracy: 0.9293 - val_accuracy: 0.4534 - val_loss: 1.2805 - val_top-5-accuracy: 0.9752\nEpoch 7/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 377ms/step - accuracy: 0.4560 - loss: 1.3823 - top-5-accuracy: 0.9257 - val_accuracy: 0.5093 - val_loss: 1.1875 - val_top-5-accuracy: 0.9627\nEpoch 8/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 393ms/step - accuracy: 0.5131 - loss: 1.2977 - top-5-accuracy: 0.9289 - val_accuracy: 0.5466 - val_loss: 1.0625 - val_top-5-accuracy: 0.9938\nEpoch 9/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 378ms/step - accuracy: 0.5362 - loss: 1.2370 - top-5-accuracy: 0.9521 - val_accuracy: 0.6335 - val_loss: 0.9434 - val_top-5-accuracy: 0.9814\nEpoch 10/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 394ms/step - accuracy: 0.5611 - loss: 1.1069 - top-5-accuracy: 0.9674 - val_accuracy: 0.6025 - val_loss: 1.0251 - val_top-5-accuracy: 0.9627\nEpoch 11/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 371ms/step - accuracy: 0.5970 - loss: 1.0434 - top-5-accuracy: 0.9658 - val_accuracy: 0.6584 - val_loss: 0.8744 - val_top-5-accuracy: 0.9689\nEpoch 12/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 371ms/step - accuracy: 0.6205 - loss: 0.9875 - top-5-accuracy: 0.9742 - val_accuracy: 0.6957 - val_loss: 0.8053 - val_top-5-accuracy: 0.9752\nEpoch 13/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 387ms/step - accuracy: 0.6445 - loss: 0.8769 - top-5-accuracy: 0.9814 - val_accuracy: 0.6894 - val_loss: 0.7418 - val_top-5-accuracy: 0.9814\nEpoch 14/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 372ms/step - accuracy: 0.6805 - loss: 0.8541 - top-5-accuracy: 0.9838 - val_accuracy: 0.8137 - val_loss: 0.6415 - val_top-5-accuracy: 0.9876\nEpoch 15/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 369ms/step - accuracy: 0.6843 - loss: 0.7988 - top-5-accuracy: 0.9928 - val_accuracy: 0.8385 - val_loss: 0.5528 - val_top-5-accuracy: 0.9814\nEpoch 16/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 386ms/step - accuracy: 0.7263 - loss: 0.7060 - top-5-accuracy: 0.9918 - val_accuracy: 0.8261 - val_loss: 0.4682 - val_top-5-accuracy: 0.9938\nEpoch 17/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 368ms/step - accuracy: 0.7635 - loss: 0.6428 - top-5-accuracy: 0.9924 - val_accuracy: 0.8634 - val_loss: 0.4227 - val_top-5-accuracy: 0.9876\nEpoch 18/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 376ms/step - accuracy: 0.7896 - loss: 0.5591 - top-5-accuracy: 0.9973 - val_accuracy: 0.8882 - val_loss: 0.4185 - val_top-5-accuracy: 1.0000\nEpoch 19/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 366ms/step - accuracy: 0.8245 - loss: 0.4922 - top-5-accuracy: 0.9985 - val_accuracy: 0.8944 - val_loss: 0.3550 - val_top-5-accuracy: 1.0000\nEpoch 20/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 366ms/step - accuracy: 0.8417 - loss: 0.4193 - top-5-accuracy: 0.9991 - val_accuracy: 0.9068 - val_loss: 0.3550 - val_top-5-accuracy: 0.9938\nEpoch 21/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 376ms/step - accuracy: 0.8675 - loss: 0.3914 - top-5-accuracy: 0.9972 - val_accuracy: 0.8447 - val_loss: 0.4044 - val_top-5-accuracy: 0.9938\nEpoch 22/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 367ms/step - accuracy: 0.8764 - loss: 0.3763 - top-5-accuracy: 0.9988 - val_accuracy: 0.8882 - val_loss: 0.3196 - val_top-5-accuracy: 1.0000\nEpoch 23/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 392ms/step - accuracy: 0.8667 - loss: 0.3455 - top-5-accuracy: 1.0000 - val_accuracy: 0.8820 - val_loss: 0.3313 - val_top-5-accuracy: 0.9938\nEpoch 24/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 371ms/step - accuracy: 0.8867 - loss: 0.3140 - top-5-accuracy: 1.0000 - val_accuracy: 0.9130 - val_loss: 0.3089 - val_top-5-accuracy: 0.9938\nEpoch 25/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 370ms/step - accuracy: 0.8856 - loss: 0.3394 - top-5-accuracy: 0.9981 - val_accuracy: 0.8571 - val_loss: 0.4245 - val_top-5-accuracy: 0.9938\nEpoch 26/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 392ms/step - accuracy: 0.8565 - loss: 0.4000 - top-5-accuracy: 0.9978 - val_accuracy: 0.8571 - val_loss: 0.3962 - val_top-5-accuracy: 0.9876\nEpoch 27/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 371ms/step - accuracy: 0.9044 - loss: 0.2691 - top-5-accuracy: 1.0000 - val_accuracy: 0.9317 - val_loss: 0.3153 - val_top-5-accuracy: 0.9814\nEpoch 28/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 372ms/step - accuracy: 0.9182 - loss: 0.2389 - top-5-accuracy: 1.0000 - val_accuracy: 0.8944 - val_loss: 0.3809 - val_top-5-accuracy: 1.0000\nEpoch 29/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 389ms/step - accuracy: 0.9129 - loss: 0.2602 - top-5-accuracy: 1.0000 - val_accuracy: 0.8882 - val_loss: 0.3663 - val_top-5-accuracy: 1.0000\nEpoch 30/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 373ms/step - accuracy: 0.9390 - loss: 0.1900 - top-5-accuracy: 1.0000 - val_accuracy: 0.9130 - val_loss: 0.3271 - val_top-5-accuracy: 0.9938\nEpoch 31/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 372ms/step - accuracy: 0.9394 - loss: 0.1732 - top-5-accuracy: 1.0000 - val_accuracy: 0.8820 - val_loss: 0.3910 - val_top-5-accuracy: 0.9938\nEpoch 32/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 387ms/step - accuracy: 0.9715 - loss: 0.1005 - top-5-accuracy: 1.0000 - val_accuracy: 0.8385 - val_loss: 0.6115 - val_top-5-accuracy: 0.9938\nEpoch 33/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 367ms/step - accuracy: 0.9390 - loss: 0.1842 - top-5-accuracy: 0.9960 - val_accuracy: 0.8882 - val_loss: 0.3178 - val_top-5-accuracy: 1.0000\nEpoch 34/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 369ms/step - accuracy: 0.9660 - loss: 0.1112 - top-5-accuracy: 1.0000 - val_accuracy: 0.9006 - val_loss: 0.3093 - val_top-5-accuracy: 1.0000\nEpoch 35/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 388ms/step - accuracy: 0.9780 - loss: 0.0836 - top-5-accuracy: 1.0000 - val_accuracy: 0.9068 - val_loss: 0.2763 - val_top-5-accuracy: 1.0000\nEpoch 36/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 372ms/step - accuracy: 0.9771 - loss: 0.0849 - top-5-accuracy: 1.0000 - val_accuracy: 0.9006 - val_loss: 0.3329 - val_top-5-accuracy: 1.0000\nEpoch 37/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 382ms/step - accuracy: 0.9839 - loss: 0.0600 - top-5-accuracy: 1.0000 - val_accuracy: 0.9193 - val_loss: 0.3402 - val_top-5-accuracy: 0.9938\nEpoch 38/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 371ms/step - accuracy: 0.9819 - loss: 0.0504 - top-5-accuracy: 1.0000 - val_accuracy: 0.9130 - val_loss: 0.3257 - val_top-5-accuracy: 1.0000\nEpoch 39/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 372ms/step - accuracy: 0.9898 - loss: 0.0582 - top-5-accuracy: 1.0000 - val_accuracy: 0.9193 - val_loss: 0.3212 - val_top-5-accuracy: 0.9938\nEpoch 40/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 388ms/step - accuracy: 0.9936 - loss: 0.0338 - top-5-accuracy: 1.0000 - val_accuracy: 0.9068 - val_loss: 0.3976 - val_top-5-accuracy: 0.9938\nEpoch 41/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 374ms/step - accuracy: 0.9900 - loss: 0.0377 - top-5-accuracy: 1.0000 - val_accuracy: 0.9068 - val_loss: 0.4275 - val_top-5-accuracy: 0.9938\nEpoch 42/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 373ms/step - accuracy: 0.9785 - loss: 0.0521 - top-5-accuracy: 1.0000 - val_accuracy: 0.8758 - val_loss: 0.5247 - val_top-5-accuracy: 1.0000\nEpoch 43/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 392ms/step - accuracy: 0.9706 - loss: 0.0688 - top-5-accuracy: 1.0000 - val_accuracy: 0.9441 - val_loss: 0.2797 - val_top-5-accuracy: 0.9938\nEpoch 44/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 373ms/step - accuracy: 0.9975 - loss: 0.0221 - top-5-accuracy: 1.0000 - val_accuracy: 0.9006 - val_loss: 0.3272 - val_top-5-accuracy: 1.0000\nEpoch 45/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 376ms/step - accuracy: 1.0000 - loss: 0.0133 - top-5-accuracy: 1.0000 - val_accuracy: 0.9130 - val_loss: 0.2968 - val_top-5-accuracy: 1.0000\nEpoch 46/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 391ms/step - accuracy: 1.0000 - loss: 0.0085 - top-5-accuracy: 1.0000 - val_accuracy: 0.9130 - val_loss: 0.3135 - val_top-5-accuracy: 1.0000\nEpoch 47/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 371ms/step - accuracy: 1.0000 - loss: 0.0078 - top-5-accuracy: 1.0000 - val_accuracy: 0.9193 - val_loss: 0.3209 - val_top-5-accuracy: 1.0000\nEpoch 48/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 372ms/step - accuracy: 1.0000 - loss: 0.0052 - top-5-accuracy: 1.0000 - val_accuracy: 0.9317 - val_loss: 0.2963 - val_top-5-accuracy: 1.0000\nEpoch 49/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 390ms/step - accuracy: 1.0000 - loss: 0.0039 - top-5-accuracy: 1.0000 - val_accuracy: 0.9441 - val_loss: 0.2767 - val_top-5-accuracy: 1.0000\nEpoch 50/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 374ms/step - accuracy: 1.0000 - loss: 0.0028 - top-5-accuracy: 1.0000 - val_accuracy: 0.9379 - val_loss: 0.2901 - val_top-5-accuracy: 1.0000\nEpoch 51/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 386ms/step - accuracy: 1.0000 - loss: 0.0025 - top-5-accuracy: 1.0000 - val_accuracy: 0.9441 - val_loss: 0.2865 - val_top-5-accuracy: 1.0000\nEpoch 52/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 378ms/step - accuracy: 1.0000 - loss: 0.0022 - top-5-accuracy: 1.0000 - val_accuracy: 0.9441 - val_loss: 0.2948 - val_top-5-accuracy: 1.0000\nEpoch 53/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 376ms/step - accuracy: 1.0000 - loss: 0.0022 - top-5-accuracy: 1.0000 - val_accuracy: 0.9379 - val_loss: 0.2960 - val_top-5-accuracy: 1.0000\nEpoch 54/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 401ms/step - accuracy: 1.0000 - loss: 0.0021 - top-5-accuracy: 1.0000 - val_accuracy: 0.9379 - val_loss: 0.2942 - val_top-5-accuracy: 1.0000\nEpoch 55/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 378ms/step - accuracy: 1.0000 - loss: 0.0019 - top-5-accuracy: 1.0000 - val_accuracy: 0.9379 - val_loss: 0.2941 - val_top-5-accuracy: 1.0000\nEpoch 56/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 375ms/step - accuracy: 1.0000 - loss: 0.0018 - top-5-accuracy: 1.0000 - val_accuracy: 0.9379 - val_loss: 0.2940 - val_top-5-accuracy: 1.0000\nEpoch 57/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 390ms/step - accuracy: 1.0000 - loss: 0.0018 - top-5-accuracy: 1.0000 - val_accuracy: 0.9379 - val_loss: 0.2999 - val_top-5-accuracy: 1.0000\nEpoch 58/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 377ms/step - accuracy: 1.0000 - loss: 0.0017 - top-5-accuracy: 1.0000 - val_accuracy: 0.9317 - val_loss: 0.2965 - val_top-5-accuracy: 1.0000\nEpoch 59/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 380ms/step - accuracy: 1.0000 - loss: 0.0015 - top-5-accuracy: 1.0000 - val_accuracy: 0.9317 - val_loss: 0.3012 - val_top-5-accuracy: 1.0000\nEpoch 60/60\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 394ms/step - accuracy: 1.0000 - loss: 0.0016 - top-5-accuracy: 1.0000 - val_accuracy: 0.9379 - val_loss: 0.3045 - val_top-5-accuracy: 1.0000\n\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.7998 - loss: 0.9852 - top-5-accuracy: 0.9813\nTest accuracy: 80.16%\nTest top 5 accuracy: 97.7%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"I5d71c0Jw4Hw"}},{"cell_type":"code","source":"NUM_SAMPLES_VIZ = 25\ntestsamples, labels = next(iter(testloader))\ntestsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n\nground_truths = []\npreds = []\nvideos = []\n\nfor i, (testsample, label) in enumerate(zip(testsamples, labels)):\n    # Generate gif\n    testsample = np.reshape(testsample.numpy(), (-1, 28, 28))\n    with io.BytesIO() as gif:\n        imageio.mimsave(gif, (testsample * 255).astype(\"uint8\"), \"GIF\", fps=5)\n        videos.append(gif.getvalue())\n\n    # Get model prediction\n    output = model.predict(ops.expand_dims(testsample, axis=0))[0]\n    pred = np.argmax(output, axis=0)\n\n    ground_truths.append(label.numpy().astype(\"int\"))\n    preds.append(pred)\n\n\ndef make_box_for_grid(image_widget, fit):\n    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n\n    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n    \"\"\"\n    # Make the caption\n    if fit is not None:\n        fit_str = \"'{}'\".format(fit)\n    else:\n        fit_str = str(fit)\n\n    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n\n    # Make the green box with the image widget inside it\n    boxb = ipywidgets.widgets.Box()\n    boxb.children = [image_widget]\n\n    # Compose into a vertical box\n    vb = ipywidgets.widgets.VBox()\n    vb.layout.align_items = \"center\"\n    vb.children = [h, boxb]\n    return vb\n\n\nboxes = []\nfor i in range(NUM_SAMPLES_VIZ):\n    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n    true_class = info[\"label\"][str(ground_truths[i])]\n    pred_class = info[\"label\"][str(preds[i])]\n    caption = f\"T: {true_class} | P: {pred_class}\"\n\n    boxes.append(make_box_for_grid(ib, caption))\n\nipywidgets.widgets.GridBox(\n    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n)","metadata":{"id":"Q16a9AdUw4Hw","executionInfo":{"status":"aborted","timestamp":1739932267670,"user_tz":-420,"elapsed":103414,"user":{"displayName":"Ha Vu Thi Thu","userId":"05654513345152712085"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T03:18:07.422825Z","iopub.execute_input":"2025-02-19T03:18:07.423099Z","iopub.status.idle":"2025-02-19T03:18:11.519271Z","shell.execute_reply.started":"2025-02-19T03:18:07.423077Z","shell.execute_reply":"2025-02-19T03:18:11.517425Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"GridBox(children=(VBox(children=(HTML(value=\"'T: pancreas | P: pancreas'\"), Box(children=(Image(value=b'GIF89a…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f4e55069b83446a96b58919f8371c18"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Final thoughts\n\nWith a vanilla implementation, we achieve ~79-80% Top-1 accuracy on the\ntest dataset.\n\nThe hyperparameters used in this tutorial were finalized by running a\nhyperparameter search using\n[W&B Sweeps](https://docs.wandb.ai/guides/sweeps).\nYou can find out our sweeps result\n[here](https://wandb.ai/minimal-implementations/vivit/sweeps/66fp0lhz)\nand our quick analysis of the results\n[here](https://wandb.ai/minimal-implementations/vivit/reports/Hyperparameter-Tuning-Analysis--VmlldzoxNDEwNzcx).\n\nFor further improvement, you could look into the following:\n\n- Using data augmentation for videos.\n- Using a better regularization scheme for training.\n- Apply different variants of the transformer model as in the paper.\n\nWe would like to thank [Anurag Arnab](https://anuragarnab.github.io/)\n(first author of ViViT) for helpful discussion. We are grateful to\n[Weights and Biases](https://wandb.ai/site) program for helping with\nGPU credits.\n\nYou can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/video-vision-transformer)\nand try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/video-vision-transformer-CT).","metadata":{"id":"_GqXaBsFw4Hx"}}]}